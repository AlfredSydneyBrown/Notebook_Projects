# -*- coding: utf-8 -*-
"""Housing_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PbgYZ9QfdxDeDPOCyakWQJChTGnEEtj3
"""

from google.colab import drive

drive.mount('/content/gdrive')

"""##Installing necessary libraries and accessing correct drive-folder"""

!pip install geopandas
!apt-get -qq install python-cartopy python3-cartopy
import cartopy
!pip install geoplot

# Commented out IPython magic to ensure Python compatibility.
import geopandas as gpd
import geoplot as gplt
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('seaborn')

# %matplotlib inline

cd "/content/gdrive/My Drive/Colab Notebooks/Housing Market Analysis"

"""#Data collection

In the following we collect the necessary data to generate a "map" of berlin (geocoded information). Please see https://juanitorduz.github.io/germany_plots/ for more details on where you can get this data, but I'll make it available on my repository too.

##Getting geolocation data for visualisation purposes

All the different postcodes of Germany:
"""

plz_shape_df = gpd.read_file('Data/plz-gebiete.shp', dtype={'plz': str})

plz_shape_df.head()

"""Getting data on which postcodes belong to which bundesland of Germany:

"""

plz_region_df = pd.read_csv(
    'Data/zuordnung_plz_ort.csv', 
    sep=',', 
    dtype={'plz': str}
)

plz_region_df.drop('osm_id', axis=1, inplace=True)

plz_region_df.tail()

"""Now let's merge these together into one Berlin dataframe, that has the essential details for visualising the data:"""

#let's write a function that allows us to merge multiple datasets and how we wish them to be merged (see pandas documents)

def merge_datasets(list_of_data_sets, on_which, list_of_hows):

  for i in range(0, len(list_of_data_sets) - 1):

    dataframe = pd.merge(left=list_of_data_sets[i], right=list_of_data_sets[i + 1], on=on_which[i], how=list_of_hows[i])
    
    list_of_data_sets.pop(i)
    list_of_data_sets.insert(i + 1, dataframe)

  return dataframe

# germany_df = merge_datasets([plz_shape_df, plz_region_df], ['plz'], ['inner'])

# #let's drop the 'note' and 'bundesland' category
# germany_df.drop(['note', 'bundesland'], axis=1, inplace=True)
# germany_df.tail()

# #and create a data frame just for berlin
# berlin_df = germany_df.query('ort == "Berlin"')

# #and merge the district names as well
# berlin_df = merge_datasets([berlin_df, berlin_plz_area_df], ['plz'], ['left'])



berlin_df = merge_datasets([plz_shape_df, plz_region_df], ['plz'], ['inner']).query('ort == "Berlin"')

berlin_df.drop(['note', 'bundesland'], axis=1, inplace=True)

berlin_df.head()

"""Let's briefly plot the geodata - looking good."""

fig, ax = plt.subplots(figsize=(16,11))

berlin_df.plot(
    ax=ax, 
    column='plz', 
    cmap= 'Spectral_r',
    edgecolor='black',
    linewidth=0.3
)

ax.set(
    title='Berlin Postcodes',
    aspect=1.3
);

"""(Let's also get data on the different Bezirke, in case we want to see the data in terms of those as well - see Dr. Juan Camilo Orduz's notebook for how he got this airbnb data)


"""

berlin_neighbourhoods_df = gpd.read_file('Data/neighbourhoods.geojson')

berlin_neighbourhoods_df = berlin_neighbourhoods_df \
    [~ berlin_neighbourhoods_df['neighbourhood_group'].isnull()]

districts_df = berlin_neighbourhoods_df.dissolve(by='neighbourhood_group')
districts_df.head()

"""##Now let's collect data that might be useful for making a decision on where to buy property in Berlin.

Here we get population data per postcode address. Again see Dr. Orduz's notebook for the dataset.
"""

plz_einwohner_df = pd.read_csv(
    'Data/plz_einwohner.csv', 
    sep=',', 
    dtype={'plz': str, 'einwohner': int}
)

plz_einwohner_df.head()

"""**Now we get to the exciting part - getting up-to-date apartment buying prices across the city**

The following code was used to collect data (current, average buying-price for flats, and average buying-price over the last three years) from the real estate website https://www.homeday.de/de/preisatlas/berlin. The data is publicly available and the following selenium-based code just automated the process of collecting and storing the data. I had to run the code locally, because google colab was being difficult with selenium.
"""

# from selenium import webdriver
# from selenium.webdriver.common.keys import Keys
# import pandas as pd
# PATH = r"C:[...]\chromedriver.exe" #note must be changed to one's own file path
# print(PATH)

# berlin_plz = ['14109', '14089', '13591', '13587', '13593', '13589', '13581', '14129', '13583', '13595', '14193', '13585', '13597', '14055', '13503', '13505', '13599', '14163', '14053', '14165', '14052', '14169', '13629', '13507', '14050', '14195', '13405', '14167', '13465', '13627', '14057', '14059', '10711', '14199', '12205', '13467', '13509', '10589', '12203', '12207', '10627', '10709', '10629', '10585', '14197', '10587', '10713', '13469', '13437', '13403', '12163', '12165', '10707', '10625', '10553', '12209', '13351', '10623', '10719', '12167', '10717', '12161', '10715', '12169', '12247', '13353', '10555', '12157', '10551', '12159', '13435', '10789', '10777', '13349', '10779', '13407', '10787', '12249', '10825', '10557', '13439', '10827', '12279', '10559', '10829', '10823', '10781', '13158', '10785', '13347', '12277', '10783', '12105', '13409', '12307', '12103', '13359', '12101', '10115', '12107', '13159', '10965', '13357', '10117', '13156', '10963', '13355', '12099', '12109', '10961', '10969', '12305', '13187', '10178', '10439', '10119', '10437', '10435', '12349', '10179', '13127', '10967', '12309', '13189', '12347', '10999', '10405', '12049', '12051', '10997', '10249', '12047', '10243', '10407', '12043', '12053', '10409', '13089', '13086', '12045', '12353', '13129', '12359', '12351', '12059', '12435', '12055', '12057', '10245', '13125', '13088', '10247', '10369', '12437', '10317', '13053', '12355', '12357', '10367', '10365', '13055', '13051', '12487', '10319', '12459', '12439', '10315', '13059', '10318', '13057', '12524', '12681', '12489', '12526', '12683', '12685', '12679', '12689', '12555', '12687', '12527', '12557', '12621', '12619', '12623', '12629', '12559', '12627', '12587', '12589']
# av_prices = []
# city_trend = []
# plz_trend = []


# for idx, i in enumerate(berlin_plz):
#     wd = webdriver.Chrome(PATH)
#     weblink = "https://www.homeday.de/de/preisatlas/berlin/{}?".format(i)
#     print(weblink)
#     print(idx + 1, "/", len(berlin_plz))
#     wd.get(weblink)
#     av_price = wd.find_elements_by_class_name("price-block__price__average")
#     price_trends = wd.find_elements_by_class_name("ct-point")

#     c_trend = []
#     for trend in price_trends[:11]:
#         c_trend += [trend.get_attribute("datavalue")]
#     city_trend.append(c_trend)

#     p_trend = []
#     for trend in price_trends[11:22]:
#         p_trend.append(trend.get_attribute("datavalue"))
#     plz_trend.append(p_trend)

#     av_prices.append(av_price[1].text[2:-5])
#     #print(av_prices)
#     wd.quit()


# dic = {'plz': berlin_plz, 'av_buy_price (€/m²)': av_prices, 'plz_buy_trend (€/m²)':plz_trend,'city_buy_trend (€/m²)':city_trend}
# buy_df = pd.DataFrame(dic)  
    
# # saving the dataframe  
# buy_df.to_csv(r'C:\Users\Admin\Desktop\Coding\buy_data.csv')

"""We now load the saved pandas csv file (which you can find on my notebook-projects folder on github) """

#extracting the data gathered by the code above
berlin_buy_info_df = pd.read_csv('Data/buy_data.csv', sep=',')

berlin_buy_info_df.drop(columns=berlin_buy_info_df.columns[[0]], axis=1, inplace=True)

berlin_buy_info_df.head()

#The plz buy trend contains the average prices for every quarter year since Q4 of 2017, and up until Q2 of this year.
#ie.
# 2017 Q4
# 2018 Q1, Q2, Q3, Q4
# 2019 Q1, Q2, Q3, Q4
# 2019 Q1, Q2,

"""Let's merge this data with the berlin_df data and the population data

"""

#We briefly need to make sure that we are comparing the correct type
plz_einwohner_df['plz']= plz_einwohner_df['plz'].astype(str)
berlin_buy_info_df['plz']= berlin_buy_info_df['plz'].astype(str)

#And now we can merge
berlin_df = merge_datasets([berlin_df, plz_einwohner_df, berlin_buy_info_df], ['plz','plz'], ['left', 'left'])

berlin_df.head(5)

#osm data
osm_df = pd.read_csv('Data/osm_metrics.csv')
osm_df.drop(columns=osm_df.columns[[0]], axis=1, inplace=True)
osm_df['plz'] = osm_df['plz'].astype(str)

osm_df.head()

#merging this with the berlin dataframe
berlin_df = merge_datasets([berlin_df, osm_df], ['plz'], ['left'])

berlin_df.head()

"""Sweet - we have some interesting data, let's visualise it.

#Visualising the data

##Plotting Preparation

Let's prepare a couple of function and parameters to make the plotting easier
"""

plt.rcParams['figure.figsize'] = [16, 11]
from IPython.display import display

def show_plot_for(category, show_n_largest=0, show_n_smallest=0, print_details=False, show_bezirke=False):
  fig, ax = plt.subplots()
  berlin_df.plot(
      ax=ax, 
      column=category, 
      categorical=False, 
      legend=True, 
      cmap='autumn_r',
      edgecolor='black',
      linewidth=0.3
  )


  if show_bezirke == True:
    districts_df.plot(ax=ax, edgecolor='k',linewidth = 1)
  

  if show_n_largest > 0:
    largest_df = berlin_df.nlargest(show_n_largest, columns=category)
    largest_df.plot(ax=ax,color='red', edgecolor='k',linewidth = 2)
    if print_details == True: 
      print("--------------- Max (colour: red) -------------")
      display(largest_df)
      print("\n")
    

  if show_n_smallest > 0:
    smallest_df = berlin_df.nsmallest(show_n_smallest, columns=category)
    smallest_df.plot(ax=ax,color='yellow', edgecolor='k',linewidth = 2)
    if print_details == True:
      print("--------------- Min (colour: yellow) -------------")
      display(smallest_df)
      print("\n")

  ax.set(
      title='Berlin:' + category, 
      aspect=1.3,
      facecolor='lightblue'
  );
  

def print_available_categories():
  for col_name in berlin_df.columns: 
    print(col_name)

"""##Plot Data

Let's rename some of the columns and see which categories we could potentially plot
"""

#Let's rename some of the columns
berlin_df.rename(columns = {'av_buy_price (€/m²)':'Average Price', 'plz_buy_trend (€/m²)':'Plz Price Trend', 'city_buy_trend (€/m²)': 'City Price Trend'}, inplace = True)

#Show available categories
print_available_categories()

"""Let's plot the 5 most expensive postcodes (red) to buy property in and the 5 cheapest (yellow)."""

show_plot_for('Average Price', show_n_largest=5, show_n_smallest=5, show_bezirke=False)

"""Interesting! What about the most populated and least populated areas?"""

show_plot_for('einwohner', show_n_largest=5, show_n_smallest=5, show_bezirke=False)

"""I did not expect that... I thought Neukölln or Mitte would have the most populated postcodes...

##Price Trends

Let us now have a quick look at which postcodes are trending the most (and the least)
"""

#Annoyingly the price trend arrays were saved as strings, so we have to convert them to lists first
def convert_price_trend_to_array(string):

  string = string.strip('[]').replace('\'', '').split(', ')

  return [int(i) for i in string]

#Now we compute the percentage growth for over as many quarters we like (default is 4, i.e. 1 year)

def compute_percentage_growth_rate(trend_array, quarters=4):
  percentages = []

  for i in range(1, len(trend_array)):

    percentages.append(((trend_array[i] - trend_array[i - 1])/trend_array[i - 1])*100)

  return sum(percentages[:quarters])

#Let's plot the most (and least) trending postcodes over the last quarter
num_quarters = 6
berlin_df['Price Percentage Growth'] = \
 [compute_percentage_growth_rate(convert_price_trend_to_array(berlin_df['City Price Trend'][i]), quarters=num_quarters) for i in range(len(berlin_df['City Price Trend']))]

show_plot_for('Price Percentage Growth', show_n_largest=1, show_n_smallest=1, print_details=True, show_bezirke=True)

"""Looks like over the last 6 quarters the apartment prices near Adlershof have grown the most...

#Finding the Optimum Postcode to Invest In

Now we move onto finding the optimal place to invest (given our own desires). To do this I created a simple least-squares cost function that converts the descrepancies between the optimal and non-optimal ouputs of the categories to a single number. The categories that result in the minimum cost function output is considered the optimal.
"""

print_available_categories()

"""Decide here what you wish to be minimsed and what to maximise. I chose that I would like to find the optimal postcode that has been trending the most since the last 8 quarters (i.e. since 2018 Q2), but at the same time also has the cheapest average buying prices."""

num_quarters = 8 #change this to see percentage increase over desired number of quarters
berlin_df['Price Percentage Growth'] = \
 [compute_percentage_growth_rate(convert_price_trend_to_array(berlin_df['City Price Trend'][i]), quarters=num_quarters) for i in range(len(berlin_df['City Price Trend']))]

dict_max = {'price trend': berlin_df['Price Percentage Growth']}
dict_min = {'average price': berlin_df['Average Price']}

"""Optimisation Functions"""

def find_min_and_max(categories_min, categories_max):
  min_opt = []
  max_opt = []

  for key, value in categories_min.items():
    min_opt.append(value.min())
  
  for key, value in categories_max.items():
    max_opt.append(value.max())

  return min_opt, max_opt

def cost_function(min_opt, max_opt, input_min_dict, input_max_dict):
  cost_min_dict = []
  cost_max_dict = []

  for i in range(len(min_opt)):
    cost_min_dict.append(((min_opt[i] - input_min_dict[i])/min_opt[i])**2)
  
  for i in range(len(max_opt)):
    cost_max_dict.append(((max_opt[i] - input_max_dict[i])/max_opt[i])**2)

  return sum(cost_min_dict) + sum(cost_max_dict)


def find_optimal_postcode(dict_min, dict_max):
  values_min = []
  for items in dict_min.values():
    values_min.append(items.tolist())

  values_max = []
  for items in dict_max.values():
    values_max.append(items.tolist())

  min_opt, max_opt = find_min_and_max(dict_min, dict_max)

  print(min_opt, max_opt)
  cost = []
  
  for i in range(len(values_max[0])):
    input_min_dict = []
    input_max_dict = []
    for j in range(len(values_min)):
      input_min_dict.append(values_min[j][i])

    for j in range(len(values_max)):
      input_max_dict.append(values_max[j][i])

    cost.append(cost_function(min_opt, max_opt, input_min_dict, input_max_dict))

  return cost, np.argmin(np.array(cost)), min(cost)

"""Finally... According to the two categories I have chosen, the best place to invest in is..."""

cost, opt_index, min_cost = find_optimal_postcode(dict_min, dict_max)

display(berlin_df.query('index == {}'.format(opt_index)))

fig, ax = plt.subplots()

berlin_df.plot(
    ax=ax, 
    categorical=False, 
    legend=True, 
    edgecolor='black',
    linewidth=0.3
)

districts_df.plot(ax=ax, edgecolor='k',linewidth = 1)

berlin_df.query('index == {}'.format(opt_index)).plot(ax=ax, edgecolor='black',
    linewidth=2)

"""... looks like Lichtenberg is place to invest in! 

"""